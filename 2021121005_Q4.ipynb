{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "improving-pepper",
      "metadata": {
        "id": "improving-pepper"
      },
      "source": [
        "# Assignment 2 - Question 4\n",
        "The objective of this assignment is to get you familiarize with  the  problem  of  `Linear Regression`.\n",
        "\n",
        "## Instructions\n",
        "- Write your code and analysis in the indicated cells.\n",
        "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
        "- Do not attempt to change the contents of other cells.\n",
        "- No inbuilt functions to be used until specified\n",
        "\n",
        "## Submission\n",
        "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
        "- Rename the notebook to `<roll_number>_Q4.ipynb`.\n",
        "- Fill the Name and Roll number in the below markdown"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f6269f1",
      "metadata": {
        "id": "6f6269f1"
      },
      "source": [
        "Name:P Shiridi Kumar <br>\n",
        "Roll Number: 2021121005"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "final-transaction",
      "metadata": {
        "id": "final-transaction"
      },
      "source": [
        "## Background about the dataset\n",
        "\n",
        "TLDR: You have 4 independent variables (`float`) for each molecule. You can use a linear combination of these 4 independent variables to predict the bandgap (dependent variable) of each molecule.\n",
        "\n",
        "You can read more about the problem in [Li et al, Bandgap tuning strategy by cations and halide ions of lead halide perovskites learned from machine learning, RSC Adv., 2021,11, 15688-15694](https://doi.org/10.1039/D1RA03117A)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "lyric-olympus",
      "metadata": {
        "id": "lyric-olympus"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "hundred-receipt",
      "metadata": {
        "id": "hundred-receipt"
      },
      "outputs": [],
      "source": [
        "all_molecules = list()\n",
        "\n",
        "with open('bg_data.txt', 'r') as infile:\n",
        "    input_rows = csv.DictReader(infile)\n",
        "    \n",
        "    for row in input_rows:\n",
        "        current_mol = ([float(row['Cs']), float(row['FA']), float(row['Cl']), float(row['Br'])], float(row['Bandgap']))\n",
        "        all_molecules.append(current_mol)\n",
        "\n",
        "random.shuffle(all_molecules)\n",
        "\n",
        "\n",
        "num_train = int(len(all_molecules) * 0.8)\n",
        "\n",
        "# each point in x_train has 4 values - 1 for each feature\n",
        "x_train = [x[0] for x in all_molecules[:num_train]]\n",
        "# each point in y_train has 1 value - the bandgap of the molecule\n",
        "y_train = [x[1] for x in all_molecules[:num_train]]\n",
        "\n",
        "x_test = [x[0] for x in all_molecules[num_train:]]\n",
        "y_test = [x[1] for x in all_molecules[num_train:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "square-direction",
      "metadata": {
        "id": "square-direction"
      },
      "source": [
        "### 4.1 Implement a Linear Regression model that minimizes the MSE **without using any libraries**. You may use NumPy to vectorize your code, but *do not use numpy.polyfit* or anything similar.\n",
        "\n",
        "4.1.1 Explain how you plan to implement Linear Regression in 5-10 lines."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 4.1.1) Linear regression can be implemented by finding a line quation which best fit the training sample and could be generalized to any data make predictions . In order to find the best fit line we can follow gradient descent optimization by taking the loss function as MSE(i.e, the mean square of the difference between the predicted and actual points) as we iteratively update the weights , the algorithm might eventually converge and the final weights and bias are used as the coefficients of the line equation."
      ],
      "metadata": {
        "id": "iJmId6YNZSA9"
      },
      "id": "iJmId6YNZSA9"
    },
    {
      "cell_type": "markdown",
      "id": "addressed-winter",
      "metadata": {
        "id": "addressed-winter"
      },
      "source": [
        "4.1.2 Implement Linear Regression using `x_train` and `y_train` as the train dataset.\n",
        "\n",
        "4.1.2.1 Choose the best learning rate and print the learning rate for which you achieved the best MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "angry-depression",
      "metadata": {
        "id": "angry-depression"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class LinearRegression:\n",
        "\n",
        "  def __init__(self,lr=0.1,maxiter=1000):\n",
        "    self.lr=lr\n",
        "    self.maxiter=maxiter\n",
        "\n",
        "  def fit(self,xtrain,ytrain):\n",
        "\n",
        "    self.xtrain=xtrain\n",
        "    self.ytrain=ytrain\n",
        "\n",
        "    self.padx()\n",
        "    params=self.init_params(xtrain)\n",
        "    self.params=params\n",
        "    self.GradientDescent()\n",
        "    return self.params\n",
        "  \n",
        "  def getparams(self):\n",
        "    return self.params\n",
        "  \n",
        "  def padx(self):\n",
        "    bias=np.ones(len(self.xtrain))\n",
        "    bias=bias.reshape(len(self.xtrain),1)\n",
        "    self.xtrain=np.concatenate((self.xtrain,bias),axis=1)\n",
        "  \n",
        "  def concatenate(self,vals):\n",
        "    bias=np.ones(len(vals))\n",
        "    bias=bias.reshape(len(vals),1)\n",
        "    vals=np.concatenate((vals,bias),axis=1)\n",
        "    return vals\n",
        "    \n",
        "  def init_params(self,xtrain):\n",
        "    params=np.random.randn(len(self.xtrain[0]), 1)\n",
        "    return params\n",
        "  \n",
        "  def predict(self,xtest):\n",
        "    xtest=np.array(xtest)\n",
        "    if(xtest.shape[1]!=self.params.shape[0]):\n",
        "      xtest=self.concatenate(xtest)\n",
        "    if(xtest.ndim!=2):\n",
        "      raise ValueError(\"Expected 2D array found 1D array\")\n",
        "    return sum(xtest.T*self.params)\n",
        "  \n",
        "  def calc_loss(self,output):\n",
        "    dz=sum((1/len(self.ytrain))*((output-self.ytrain)**2).reshape(-1,1))\n",
        "    return dz\n",
        "\n",
        "  def update_weights(self,outputs):\n",
        "    dz=(outputs-self.ytrain).reshape(-1,1)\n",
        "    self.params-=self.lr*(1/len(self.xtrain))*sum(self.xtrain*dz).reshape(-1,1)\n",
        "\n",
        "  \n",
        "  def GradientDescent(self):\n",
        "    for i in range(self.maxiter):\n",
        "      h_x=sum((self.xtrain*self.params.T).T)\n",
        "      self.calc_loss(h_x)\n",
        "      self.update_weights(h_x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "angry-tampa",
      "metadata": {
        "id": "angry-tampa"
      },
      "source": [
        "4.1.3 Make a [Parity Plot](https://en.wikipedia.org/wiki/Parity_plot) of your model's bandgap predictions on the test set with the actual values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "foster-center",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 673
        },
        "id": "foster-center",
        "outputId": "719302e4-e303-4052-f860-f559b917dce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best value of learning rate which gave best RMSE most of the times =  0.1 \n",
            "RMSE : 0.05981049221240238\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x1440 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Get the predictions of x_test into `y_pred`\n",
        "\n",
        "#\n",
        "# ...\n",
        "#\n",
        "\n",
        "def calc_mse(ypred,ytest):\n",
        "  return math.sqrt(sum((ypred-ytest)**2)/len(ypred))\n",
        "# for i in range(5,100,5):\n",
        "model=LinearRegression(lr=0.1,maxiter=1000)\n",
        "model.fit(x_train,y_train)\n",
        "y_pred=model.predict(x_test)\n",
        "print(\"Best value of learning rate which gave best RMSE most of the times = \",0.1,\"\\nRMSE :\",calc_mse(y_pred,y_test))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(10,20))\n",
        "\n",
        "ax.scatter(y_test, y_pred)\n",
        "\n",
        "lims = [\n",
        "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
        "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
        "]\n",
        "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
        "ax.set_aspect('equal')\n",
        "ax.set_xlim(lims)\n",
        "ax.set_ylim(lims)\n",
        "\n",
        "ax.set_title('Parity Plot of Custom Linear Regression')\n",
        "ax.set_xlabel('Ground truth bandgap values')\n",
        "ax.set_ylabel('Predicted bandgap values')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dominant-chaos",
      "metadata": {
        "id": "dominant-chaos"
      },
      "source": [
        "### 4.2 Implement Ridge regression\n",
        "4.2.1 Explain Ridge regression briefly in 1-2 lines."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "happy-cyprus",
      "metadata": {
        "id": "happy-cyprus"
      },
      "source": [
        "<!-- Your answer to 1.2.1 -->\n",
        "A 4.2.1) Many a times the model might tend to overfit because of complex weights . Ridge regression adds the sum of the square of the weights to the loss function and because of which the gradient descent algorithm tries to update the weights in such a way that the magnitude of the weights is as low as possible and have a simpler model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tight-forwarding",
      "metadata": {
        "id": "tight-forwarding"
      },
      "source": [
        "4.2.2 Implement Ridge regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total. (You can use scikit-learn from this cell onwards)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 4.2.2) the parameter alpha controls the effect of the of the sum of the squares of the weights added to the loss function . Alpha is multipled with the sum of the sqaures of the weights and added to loss function. So ideally when alpha is =0 then the effect of the term would be 0 and the loss function woudld be same as the normal linear regression and in this case ridge regression performs similar to linear regression. \n",
        "We can infer the same from the below table as well . when the alpha value is very low or 0 its rmse is similar to linear regression in the above section as alpha increases the effect of the sum of the squares of the weights increases in loss function and hence by the algorithm eventually tries to decrease the magnitude of the weights as less as possible . so ideally in most of the cases as alpha increases the sum of the squares of the weights also decreases and we can infer the same from the below table ."
      ],
      "metadata": {
        "id": "T3S8pYHqcAyd"
      },
      "id": "T3S8pYHqcAyd"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "violent-northern",
      "metadata": {
        "id": "violent-northern",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8f98b771-24c2-46ea-e3c8-2aaec3c69331"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Alpha value      RMSE  Sum of squares of the coeffecients\n",
              "0          0.00  0.059299                            2.745944\n",
              "1          0.05  0.060245                            2.688918\n",
              "2          0.10  0.061347                            2.633873\n",
              "3          0.15  0.062586                            2.580714\n",
              "4          0.20  0.063942                            2.529352\n",
              "5          0.25  0.065399                            2.479704\n",
              "6          0.30  0.066941                            2.431691\n",
              "7          0.35  0.068554                            2.385240\n",
              "8          0.40  0.070225                            2.340279\n",
              "9          0.45  0.071945                            2.296744\n",
              "10         0.50  0.073704                            2.254572\n",
              "11         0.55  0.075493                            2.213704\n",
              "12         0.60  0.077305                            2.174083\n",
              "13         0.65  0.079135                            2.135659\n",
              "14         0.70  0.080976                            2.098380\n",
              "15         0.75  0.082824                            2.062200\n",
              "16         0.80  0.084675                            2.027073\n",
              "17         0.85  0.086527                            1.992957\n",
              "18         0.90  0.088375                            1.959812\n",
              "19         0.95  0.090217                            1.927599\n",
              "20         1.00  0.092052                            1.896283\n",
              "21         1.05  0.093877                            1.865827\n",
              "22         1.10  0.095691                            1.836200\n",
              "23         1.15  0.097492                            1.807370\n",
              "24         1.20  0.099280                            1.779308\n",
              "25         1.25  0.101054                            1.751984\n",
              "26         1.30  0.102812                            1.725372\n",
              "27         1.35  0.104555                            1.699445\n",
              "28         1.40  0.106281                            1.674180\n",
              "29         1.45  0.107991                            1.649552\n",
              "30         1.50  0.109684                            1.625540\n",
              "31         1.55  0.111360                            1.602121\n",
              "32         1.60  0.113018                            1.579276\n",
              "33         1.65  0.114659                            1.556984\n",
              "34         1.70  0.116282                            1.535227\n",
              "35         1.75  0.117888                            1.513987\n",
              "36         1.80  0.119476                            1.493246\n",
              "37         1.85  0.121047                            1.472988\n",
              "38         1.90  0.122600                            1.453198\n",
              "39         1.95  0.124136                            1.433860\n",
              "40         2.00  0.125655                            1.414960"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-83e2cb2b-e92c-48ab-9a33-81ba5bf3b5b6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alpha value</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>Sum of squares of the coeffecients</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.059299</td>\n",
              "      <td>2.745944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.060245</td>\n",
              "      <td>2.688918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.061347</td>\n",
              "      <td>2.633873</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.062586</td>\n",
              "      <td>2.580714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.063942</td>\n",
              "      <td>2.529352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.065399</td>\n",
              "      <td>2.479704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.066941</td>\n",
              "      <td>2.431691</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.068554</td>\n",
              "      <td>2.385240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.070225</td>\n",
              "      <td>2.340279</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.071945</td>\n",
              "      <td>2.296744</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.073704</td>\n",
              "      <td>2.254572</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.075493</td>\n",
              "      <td>2.213704</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.077305</td>\n",
              "      <td>2.174083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0.079135</td>\n",
              "      <td>2.135659</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.080976</td>\n",
              "      <td>2.098380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.082824</td>\n",
              "      <td>2.062200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.084675</td>\n",
              "      <td>2.027073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.086527</td>\n",
              "      <td>1.992957</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.088375</td>\n",
              "      <td>1.959812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.95</td>\n",
              "      <td>0.090217</td>\n",
              "      <td>1.927599</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.092052</td>\n",
              "      <td>1.896283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.05</td>\n",
              "      <td>0.093877</td>\n",
              "      <td>1.865827</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.10</td>\n",
              "      <td>0.095691</td>\n",
              "      <td>1.836200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.15</td>\n",
              "      <td>0.097492</td>\n",
              "      <td>1.807370</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.20</td>\n",
              "      <td>0.099280</td>\n",
              "      <td>1.779308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.25</td>\n",
              "      <td>0.101054</td>\n",
              "      <td>1.751984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.30</td>\n",
              "      <td>0.102812</td>\n",
              "      <td>1.725372</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.35</td>\n",
              "      <td>0.104555</td>\n",
              "      <td>1.699445</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.40</td>\n",
              "      <td>0.106281</td>\n",
              "      <td>1.674180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.45</td>\n",
              "      <td>0.107991</td>\n",
              "      <td>1.649552</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.50</td>\n",
              "      <td>0.109684</td>\n",
              "      <td>1.625540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.55</td>\n",
              "      <td>0.111360</td>\n",
              "      <td>1.602121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.60</td>\n",
              "      <td>0.113018</td>\n",
              "      <td>1.579276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.65</td>\n",
              "      <td>0.114659</td>\n",
              "      <td>1.556984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.70</td>\n",
              "      <td>0.116282</td>\n",
              "      <td>1.535227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1.75</td>\n",
              "      <td>0.117888</td>\n",
              "      <td>1.513987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.80</td>\n",
              "      <td>0.119476</td>\n",
              "      <td>1.493246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.85</td>\n",
              "      <td>0.121047</td>\n",
              "      <td>1.472988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.90</td>\n",
              "      <td>0.122600</td>\n",
              "      <td>1.453198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.95</td>\n",
              "      <td>0.124136</td>\n",
              "      <td>1.433860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2.00</td>\n",
              "      <td>0.125655</td>\n",
              "      <td>1.414960</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-83e2cb2b-e92c-48ab-9a33-81ba5bf3b5b6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-83e2cb2b-e92c-48ab-9a33-81ba5bf3b5b6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-83e2cb2b-e92c-48ab-9a33-81ba5bf3b5b6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "# you should not have imported sklearn before this point\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import Ridge\n",
        "import pandas as pd\n",
        "table=[]\n",
        "for i in range(0,205,5):\n",
        "  alp=0.01*i\n",
        "  model = Ridge(alpha=alp)\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred=model.predict(x_test)\n",
        "  table.append((round(alp,2),calc_mse(y_pred,y_test),sum((model.coef_)**2)))\n",
        "table=pd.DataFrame(table,columns=[\"Alpha value\", \"RMSE\",\"Sum of squares of the coeffecients\"])\n",
        "table\n",
        "\n",
        "\n",
        "# implement Ridge regression and make a table where you explore the effect of different values of `alpha`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adolescent-temperature",
      "metadata": {
        "id": "adolescent-temperature"
      },
      "source": [
        "### 4.3 Implement Lasso regression\n",
        "4.3.1 Explain Lasso regression briefly in 1-2 lines."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 4.3.1) As same like ridge regression lasso regression also helps to prevent from overfitting but instead of adding sum of squares of weights lasso regression adds sum of absolute values of weights . this might eventually make the weights to zero and hence by only some weights might be used and makes the model more simples and prevent from overfit."
      ],
      "metadata": {
        "id": "io7n2rFQbHRa"
      },
      "id": "io7n2rFQbHRa"
    },
    {
      "cell_type": "markdown",
      "id": "popular-wonder",
      "metadata": {
        "id": "popular-wonder"
      },
      "source": [
        "4.3.2 Implement Lasso regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A 4.3.2) the parameter alpha controls the effect of the of the sum of the absolute values  of the weights added to the loss function . Alpha is multipled with the sum of the absolute values of the weights and added to loss function. So ideally when alpha is =0 then the effect of the term would be 0 and the loss function woudld be same as the normal linear regression and in this case ridge regression performs similar to linear regression. We can infer the same from the below table as well . when the alpha value is very low or 0 its rmse is similar to linear regression in the above section .as alpha increases the loss function gives higher importance to the added term and eventually tries to decrease the value of the term as low as possible since the weight values are not scared it might be eventually become 0 at a faster rate as alpha increases."
      ],
      "metadata": {
        "id": "NlOJRCLMduVX"
      },
      "id": "NlOJRCLMduVX"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "extra-brighton",
      "metadata": {
        "id": "extra-brighton",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4b51e317-1430-4aa0-fae5-d2db376d11b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
            "  \n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_coordinate_descent.py:648: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.643e-01, tolerance: 1.801e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
            "  coef_, l1_reg, l2_reg, X, y, max_iter, tol, rng, random, positive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Alpha value      RMSE  Sum of absolute value of the coeffecients\n",
              "0          0.00  0.059299                                   2.282465\n",
              "1          0.05  0.291756                                   0.849765\n",
              "2          0.10  0.415490                                   0.119647\n",
              "3          0.15  0.435679                                   0.000000\n",
              "4          0.20  0.435679                                   0.000000\n",
              "5          0.25  0.435679                                   0.000000\n",
              "6          0.30  0.435679                                   0.000000\n",
              "7          0.35  0.435679                                   0.000000\n",
              "8          0.40  0.435679                                   0.000000\n",
              "9          0.45  0.435679                                   0.000000\n",
              "10         0.50  0.435679                                   0.000000\n",
              "11         0.55  0.435679                                   0.000000\n",
              "12         0.60  0.435679                                   0.000000\n",
              "13         0.65  0.435679                                   0.000000\n",
              "14         0.70  0.435679                                   0.000000\n",
              "15         0.75  0.435679                                   0.000000\n",
              "16         0.80  0.435679                                   0.000000\n",
              "17         0.85  0.435679                                   0.000000\n",
              "18         0.90  0.435679                                   0.000000\n",
              "19         0.95  0.435679                                   0.000000\n",
              "20         1.00  0.435679                                   0.000000\n",
              "21         1.05  0.435679                                   0.000000\n",
              "22         1.10  0.435679                                   0.000000\n",
              "23         1.15  0.435679                                   0.000000\n",
              "24         1.20  0.435679                                   0.000000\n",
              "25         1.25  0.435679                                   0.000000\n",
              "26         1.30  0.435679                                   0.000000\n",
              "27         1.35  0.435679                                   0.000000\n",
              "28         1.40  0.435679                                   0.000000\n",
              "29         1.45  0.435679                                   0.000000\n",
              "30         1.50  0.435679                                   0.000000\n",
              "31         1.55  0.435679                                   0.000000\n",
              "32         1.60  0.435679                                   0.000000\n",
              "33         1.65  0.435679                                   0.000000\n",
              "34         1.70  0.435679                                   0.000000\n",
              "35         1.75  0.435679                                   0.000000\n",
              "36         1.80  0.435679                                   0.000000\n",
              "37         1.85  0.435679                                   0.000000\n",
              "38         1.90  0.435679                                   0.000000\n",
              "39         1.95  0.435679                                   0.000000\n",
              "40         2.00  0.435679                                   0.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7d82418a-4d7b-4833-b783-40751ea5f761\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alpha value</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>Sum of absolute value of the coeffecients</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.059299</td>\n",
              "      <td>2.282465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.291756</td>\n",
              "      <td>0.849765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.415490</td>\n",
              "      <td>0.119647</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.25</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.30</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.35</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.40</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.45</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0.50</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>0.55</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0.60</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.65</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.70</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>0.75</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0.80</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0.85</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0.90</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0.95</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>1.00</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>1.05</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.10</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>1.15</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>1.20</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.25</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.30</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.35</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.40</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>1.45</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>1.50</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>1.55</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.60</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>1.65</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>1.70</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>1.75</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>1.80</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1.85</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>1.90</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>1.95</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2.00</td>\n",
              "      <td>0.435679</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7d82418a-4d7b-4833-b783-40751ea5f761')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7d82418a-4d7b-4833-b783-40751ea5f761 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7d82418a-4d7b-4833-b783-40751ea5f761');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# implement Lasso regression and make a table where you explore the effect of different values of `alpha`\n",
        "from sklearn import linear_model\n",
        "\n",
        "table=[]\n",
        "for i in range(0,205,5):\n",
        "  alp=0.01*i\n",
        "  model = linear_model.Lasso(alpha=alp)\n",
        "  model.fit(x_train, y_train)\n",
        "  y_pred=model.predict(x_test)\n",
        "  table.append((round(alp,2),calc_mse(y_pred,y_test),sum(abs(model.coef_))))\n",
        "table=pd.DataFrame(table,columns=[\"Alpha value\", \"RMSE\",\"Sum of absolute value of the coeffecients\"])\n",
        "table"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}